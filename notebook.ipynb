{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification \n",
    "\n",
    "This notebook will implement a text classficiation model. The model will predict whether comment is pro or anti brexit. 9000 youtube/twitter comments have been gathered and annotated manually. Later we will see which words have heavier weight. i.e \"#voteleave\" is a word that is more often used amongst pro brexiters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Pre-Processing\n",
    "\n",
    "Pre-processing will consist of\n",
    "- **Tokenization**\n",
    " - Extract useful items (words) from each brexit comment. More specifically *unigram* or *1-gram* Tokenization will be done where each words will be considered an item\n",
    "- **Stop word removal**\n",
    " - Remove all words (features) will most likley make the ML model perform worse. The words are called *stop words* (i.e \"the\", they, \"shouldnt\", \".\")\n",
    "- **Feature representation (bag of word)**\n",
    " - Represent each features (word) as the number of times the word occurs in each brexit comment\n",
    "\n",
    "\n",
    "The following code section will first load the data set and split it into a training and test set. The training set will be used to find best suited hyper parameter values for different estimator (**Model selection**), while the test will will be used as a final evaluation on the model (**Model evaluation**)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            Comment\n",
      "0      1                                   Brexit = Freedom\n",
      "1      0  I voted brexit, because of the lies about fund...\n",
      "2      0  Old people voted to leave. Young people voted ...\n",
      "3      0  On the contrary, poll after poll shows the peo...\n",
      "4      0  I'd rather see it not happening, but it will b...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# The file is separated by tabs. Therefor the sep argument will be set to '\\t'\n",
    "with open(\"Data/a2_train.tsv\", 'r',encoding='utf-8') as myfile:\n",
    "    df = pd.read_csv(myfile,sep='\\t',header=None)\n",
    "\n",
    "df.rename(columns={0: 'label', 1: 'Comment'},inplace=True)\n",
    "print(df.head(5))\n",
    "\n",
    "X = df['Comment'].tolist()\n",
    "y = df['label'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "#stopWords.append(\"!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count instance will be used in the pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(ngram_range=(1, 1), lowercase=True, tokenizer=tknzr.tokenize, stop_words=stopWords)\n",
    "\n",
    "#tfidf instance will be used in the pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2',smooth_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "Hyper parameter optimization will be done using Gridsearch combined with cross validation. A 10-fold approach will be done during the cross validation. Standard accuracy will be the performance metric to optmize on. This is motivated since the classes are balanced. Otherwise *balanced accuracy* would be a more appropriate performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "/home/chris/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV on perceptron done\n",
      "GridSearchCV on SVC done\n",
      "GridSearchCV on DummyClassifier done\n",
      "Perceptron best params {'perceptron__max_iter': 30, 'perceptron__tol': 0.001}  \n",
      " Perceptron best score 0.7087845674080565 \n",
      " \n",
      "\n",
      "SVC best params: {'SVC__C': 1.0, 'SVC__kernel': 'linear'}  \n",
      " SVC best score: 0.7535209573636419 \n",
      " \n",
      "\n",
      "DummyClassifier best params: {'DummyClassifier__strategy': 'most_frequent'}  \n",
      " DummyClassifier best score: 0.5115993574048778 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import the pipeline, and all of the learning algorithms\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#Use pickle to save the model\n",
    "import pickle\n",
    "\n",
    "pipeline_perceptron = Pipeline([\n",
    "    ('vect', count),\n",
    "    ('tfidf', tfidf),\n",
    "    ('perceptron', Perceptron()),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline_SVC = Pipeline([\n",
    "    ('vect', count),\n",
    "    ('tfidf', tfidf),\n",
    "    ('SVC', SVC()),\n",
    "])\n",
    "\n",
    "pipeline_DummyClassifier = Pipeline([\n",
    "    ('vect', count),\n",
    "    ('tfidf', tfidf),\n",
    "    ('DummyClassifier', DummyClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "#Perfrom grid search for each of the learning algorithms\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#Grid search on perceptron\n",
    "param_grid_perceptron = {'perceptron__max_iter':[1,10,20,30,40,50],'perceptron__tol':[0.001]}\n",
    "GridSearch_perceptron = GridSearchCV(estimator=pipeline_perceptron, param_grid = param_grid_perceptron, \n",
    "             cv=10, scoring='accuracy')\n",
    "GridSearch_perceptron.fit(X_train, y_train)\n",
    "fp = open('Model/perceptron.pickle', 'wb')\n",
    "pickle.dump(GridSearch_perceptron.best_estimator_, fp)\n",
    "fp.close()\n",
    "print(\"GridSearchCV on perceptron done\")\n",
    "\n",
    "\n",
    "#Grid search on SVC\n",
    "param_grid_SVC = {'SVC__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 5, 10.0], 'SVC__kernel': ['linear']}\n",
    "GridSearch_SVC = GridSearchCV(estimator=pipeline_SVC, param_grid = param_grid_SVC, \n",
    "             cv=10, scoring='accuracy')\n",
    "GridSearch_SVC.fit(X_train, y_train)\n",
    "fp = open('Model/SVC.pickle', 'wb')\n",
    "pickle.dump(GridSearch_SVC.best_estimator_, fp)\n",
    "fp.close()\n",
    "print(\"GridSearchCV on SVC done\")\n",
    "\n",
    "\n",
    "#Grid search on a DummyClassifier\n",
    "param_grid_DummyClassifier = {'DummyClassifier__strategy':['most_frequent']} #'uniform','stratified',\n",
    "GridSearch_DummyClassifier = GridSearchCV(estimator=pipeline_DummyClassifier, param_grid = param_grid_DummyClassifier, \n",
    "             cv=10, scoring='accuracy')\n",
    "GridSearch_DummyClassifier.fit(X_train, y_train)\n",
    "fp = open('Model/DummyClassifier.pickle', 'wb')\n",
    "pickle.dump(GridSearch_DummyClassifier.best_estimator_, fp)\n",
    "fp.close()\n",
    "print(\"GridSearchCV on DummyClassifier done\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Perceptron best params {0}  \\n Perceptron best score {1} \\n \\n\".format(GridSearch_perceptron.best_params_, \n",
    "                                                                              GridSearch_perceptron.best_score_))\n",
    "\n",
    "print(\"SVC best params: {0}  \\n SVC best score: {1} \\n \\n\".format(GridSearch_SVC.best_params_, GridSearch_SVC.best_score_))\n",
    "\n",
    "print(\"DummyClassifier best params: {0}  \\n DummyClassifier best score: {1} \\n \\n\".format(GridSearch_DummyClassifier.best_params_,\n",
    "                                                                                          GridSearch_DummyClassifier.best_score_))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Models selection\n",
    "\n",
    "\n",
    "**Perceptron best params** {'perceptron__max_iter': 30, 'perceptron__tol': 0.001}  \n",
    "**Perceptron best score**: 0.7131317315658657    \n",
    " \n",
    "\n",
    "**SVC best params:** {'SVC__C': 1.0, 'SVC__kernel': 'linear'}    \n",
    "**SVC best score:** 0.7535211267605634    \n",
    " \n",
    "\n",
    "**DummyClassifier best params:** {'DummyClassifier__strategy': 'most_frequent'}    \n",
    "**DummyClassifier best score:** 0.5115990057995029   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "The initial test set held out earlier will be evaluated in this section. A confusion matrix will be also computed to see if there is a harder time predicting on one class (i.e anti brexit comments) than the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron test set results: 0.7258200168208578\n",
      "SVC test set results: 0.7485281749369218\n",
      "Dummy Classifier test set results: 0.5126156433978133 \n",
      " \n",
      "\n",
      "Confusion matrix for perceptron: [[802 357]\n",
      " [295 924]] \n",
      " Confusion matrix for SVC: [[859 300]\n",
      " [298 921]]\n",
      " Confusion matrix for DummyClassifier: [[   0 1159]\n",
      " [   0 1219]]\n",
      "\n",
      "True negatives are anti brexit comments correctly identified\n",
      "False positive are anti-brexit Predicted as Pro brexit\n",
      "False negative are pro brexit comments predicted as anti brexit\n",
      "True positives are pro brexit comments correctly identified\n",
      "For example: Perceptron has  802 true positive, 357 false positives, 295 false negatives, and 924 true positives\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf_perceptron = pickle.load( open( \"Model/perceptron.pickle\", \"rb\" ) )\n",
    "clf_SVC = pickle.load( open( \"Model/SVC.pickle\", \"rb\" ) )\n",
    "clf_DummyClassifier = pickle.load( open( \"Model/DummyClassifier.pickle\", \"rb\" ) )\n",
    "\n",
    "#Print test set accuracy results\n",
    "print(\"Perceptron test set results: {0}\".format(clf_perceptron.score(X_test,y_test)))\n",
    "print(\"SVC test set results: {0}\".format(clf_SVC.score(X_test,y_test)))\n",
    "print(\"Dummy Classifier test set results: {0} \\n \\n\".format(clf_DummyClassifier.score(X_test,y_test)))\n",
    "\n",
    "\n",
    "#Comput an confusion matrix\n",
    "ypred_perceptron = clf_perceptron.predict(X_test)\n",
    "ypred_SVC = clf_SVC.predict(X_test)\n",
    "ypred_DummyClassifier = clf_DummyClassifier.predict(X_test)\n",
    "\n",
    "conf_matrix_perceptron = confusion_matrix(y_true=y_test, y_pred=ypred_perceptron, labels = [0,1])\n",
    "conf_matrix_SVC = confusion_matrix(y_true=y_test, y_pred=ypred_SVC , labels = [0,1])\n",
    "conf_matrix_DummyClassifier = confusion_matrix(y_true=y_test, y_pred=ypred_DummyClassifier, labels = [0,1])\n",
    "#Print conf matrix for SVC\n",
    "print('Confusion matrix for perceptron: {0} \\n Confusion matrix for SVC: {1}\\n Confusion matrix for DummyClassifier: {2}\\n'.format(conf_matrix_perceptron,\n",
    "                                                                                                                                    conf_matrix_SVC,\n",
    "                                                                                                                                    conf_matrix_DummyClassifier))\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix_perceptron.ravel()\n",
    "print(\"True negatives are anti brexit comments correctly identified\")\n",
    "print(\"False positive are anti-brexit Predicted as Pro brexit\")\n",
    "print(\"False negative are pro brexit comments predicted as anti brexit\")\n",
    "print(\"True positives are pro brexit comments correctly identified\")\n",
    "print(\"For example: Perceptron has  {0} true positive, {1} false positives, {2} false negatives, and {3} true positives\".format(tn, fp, fn, tp))\n",
    "\n",
    "\n",
    "#Trying to print important features for perceptron and SVC\n",
    "#feature_names = clf_perceptron.named_steps['vect'].get_feature_names()\n",
    "#print(yo[0:5])\n",
    "#weights = clf_perceptron.named_steps['perceptron'].coef_\n",
    "#print(weights[0:5])\n",
    "#mapped = zip(clf_perceptron.named_steps['perceptron'].coef_ , clf_perceptron.named_steps['vect'].get_feature_names())\n",
    "#Print which features the SVC thinks are important\n",
    "#for weight, fname in mapped:\n",
    "#    print(fname, weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron test set results:** 0.7232968881412952  \n",
    "**SVC test set results:** 0.7485281749369218  \n",
    "**Dummy Classifier test set results:** 0.5126156433978133   \n",
    "  \n",
    "It can be seen that both learning algorithms (SVC, perceptron) performs better than the best possible stupid dummy baseline (Most frequent class)\n",
    "\n",
    "**Confusion matrix for perceptron:**   \n",
    "[[805 354]  \n",
    " [304 915]]     \n",
    "\n",
    "\n",
    "**Confusion matrix for SVC:**  \n",
    "[[859 300]  \n",
    "[298 921]]  \n",
    "\n",
    "\n",
    "**Confusion matrix for DummyClassifier:**   \n",
    "[[   0 1159]  \n",
    "[   0 1219]]  \n",
    "\n",
    "*True negatives* are anti brexit comments correctly identified  \n",
    "*False positive* are anti-brexit Predicted as Pro brexit   \n",
    "*False negative* are pro brexit comments predicted as anti brexit  \n",
    "*True positives* are pro brexit comments correctly identified  \n",
    "\n",
    "\n",
    "For example: Perceptron has  805 true positive, 354 false positives, 304 false negatives, and 915 true positives  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the best estimators on a separate test set\n",
    "\n",
    "The optmized estimators are in the the folder named \"Models\". The *Pickle* library can be is used to load classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7485281749369218"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "clf_pipeline = pickle.load( open( \"Model/SVC.pickle\", \"rb\" ) )\n",
    "clf_pipeline.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating features\n",
    "\n",
    "The linear classifiers SVC and perceptron can rank which features (words) are most relevant for determining the classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top pro brexit\n",
      "           Words     Score\n",
      "3105          eu  3.331471\n",
      "5276       media  2.698867\n",
      "5266       means  2.639581\n",
      "6863   remainers  2.439756\n",
      "91    #voteleave  2.360061\n",
      "2092     corrupt  2.291570\n",
      "4028        hard  2.264120\n",
      "2417  democratic  2.192142\n",
      "2415   democracy  2.124605\n",
      "4885        laws  2.003526\n",
      "Top Anti brexit\n",
      "            Words     Score\n",
      "7830     stopping -1.796267\n",
      "5686     nonsense -1.821959\n",
      "1333     brexshit -1.892504\n",
      "1317       brexit -1.920070\n",
      "93    #voteremain -1.926407\n",
      "6061        peace -2.112792\n",
      "7899       stupid -2.227987\n",
      "4915      leavers -2.437080\n",
      "1322   brexiteers -2.621479\n",
      "1325    brexiters -2.701223\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline = pickle.load( open( \"Model/SVC.pickle\", \"rb\" ) )\n",
    "featuresSparse = clf_pipeline['SVC'].coef_\n",
    "features = featuresSparse.todense()\n",
    "\n",
    "\n",
    "countVectorizer = clf_pipeline['vect']\n",
    "words = countVectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data={'Words': words, 'Score': features.tolist()[0][:]})\n",
    "df_proBrexit = df[df.Score > 0]\n",
    "df_AntiBrexit = df[df.Score < 0]\n",
    "\n",
    "df_proBrexit = df_proBrexit.sort_values(by=['Score'], ascending=False)\n",
    "df_AntiBrexit = df_AntiBrexit.sort_values(by=['Score'], ascending=False)\n",
    "print(\"Top pro brexit\")\n",
    "print(df_proBrexit.head(10))\n",
    "print(\"Top Anti brexit\")\n",
    "print(df_AntiBrexit.tail(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the annotation\n",
    "\n",
    "There are several annotators on one sample. However since the third annotator is missing on some of the samples, only an analysis of two annotators will be made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8649101431617423\n",
      "0.7408589591450898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# The file is separated by tabs. Therefor the sep argument will be set to '\\t'\n",
    "with open(\"Data/a2_final.csv\", 'r',encoding='utf-8') as myfile:\n",
    "    df = pd.read_csv(myfile,sep=',')\n",
    "\n",
    "df.rename(columns={0: 'labels', 1: 'Comment', 2: 'Anotator 1', 3: 'Anotator 2', 4: 'Anotator 3'},inplace=True)\n",
    "#print(df)\n",
    "\n",
    "df_two_annotators  = df[['Anotator 1', 'Anotator 2']].copy()\n",
    "#print(df_two_annotators)\n",
    "df_two_annotators = df_two_annotators.dropna()\n",
    "Anotator_1 = df_two_annotators['Anotator 1'].tolist()\n",
    "Anotator_2 = df_two_annotators['Anotator 2'].tolist()\n",
    "\n",
    "print(accuracy_score(Anotator_1, Anotator_2))\n",
    "print(cohen_kappa_score(Anotator_1, Anotator_2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** 0.8649101431617423  \n",
    "**Cohens kappa:** 0.7408589591450898"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
